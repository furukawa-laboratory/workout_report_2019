# kNNとベイズ誤り率
## kNNとは
kNN(k最近傍法)とは，教師あり学習の一手法であり，観測対象のクラスラベルを観測対象の近傍サンプルk個内での多数決で決めようというシンプルな手法です．$k$ は自由に決めることができるハイパーパラメータです．

以下にkNNによるクラスタリングのイメージを示します．
![](https://i.imgur.com/dkGuRIZ.png =400x400)

予め，赤，青，黄の三つのクラスに分類されたサンプルが与えられているとします．
今，クラスの分からない新たなサンプルが観測されました．図では白色になってるサンプルです．

このクラスが未知のサンプルのクラスを，kNNによって決定します．

今回はハイパーパラメータ $k=4$ とします．
$k=4$ としたので，クラス未知のサンプルの最近傍サンプルを$4$つ選びます．以下のようになります．
![](https://i.imgur.com/WbhqVlA.png =400x400)

クラスが未知のサンプルの$4$つの最近傍サンプルの内訳は，赤：$0$，青：$3$，黄：$1$となりました．多数決の結果，新たなサンプルのクラスは青に決定しました．おめでたいですね！

## 近傍の定義
上記のクラスタリングの例では，特に何の説明もせずに最近傍サンプルを決めていましたが，実は「ユークリッド距離」というピタゴラスの公式によって与えられる二点間の距離が小さい順に最近傍サンプルを決めていました．

kNNにおいては，「ユークリッド距離」を使うことが一般的とされていますが，明示しているのであれば，その他の距離を用いても問題ありません．

## 識別のリジェクト
$k$個の最近傍サンプルの内訳がすべて同じ数，つまり多数決で決定できないときはどうなるでしょうか．これについては，識別を放棄（リジェクト）する，もしくは完全にランダムに決定するといったような解決策がとられます．

## kNNの識別ルール
kNNの識別ルールを定式化すると以下のようになります．

$$
識別クラス = 
        \begin{cases}
            j \quad &\mbox{where} \quad \{k_j\}=\mbox{max}\{k_1,...,k_K\} \\
            reject \quad &\mbox{where} \quad \{k_i,...,k_j\}=\mbox{max}\{k_1,...,k_K\} \\
        \end{cases}
$$
$$
k_i : 近傍k個のサンプルのうち，クラスjに属するサンプルの数
$$

## kNNの弱点
アルゴリズムが非常にシンプルなので，実装も用意なkNNですが弱点もあります．それは，$k$ 個の最近傍サンプルを発見するために，データ空間にあるすべてのサンプルとの距離を計算しなければいけないことに起因します．全探索必須なクラスタリング手法であるため，処理が重くなりがちです．

## 次元の呪い，ベイズ誤り率の上限と下限